{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8003852,"sourceType":"datasetVersion","datasetId":4713606}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Subset, DataLoader\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport wandb\nimport time\nimport torchvision.models as models\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-05T04:18:47.295177Z","iopub.execute_input":"2024-04-05T04:18:47.295877Z","iopub.status.idle":"2024-04-05T04:18:47.303459Z","shell.execute_reply.started":"2024-04-05T04:18:47.295840Z","shell.execute_reply":"2024-04-05T04:18:47.302288Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"wandb.login(key=\"62cfafb7157dfba7fdd6132ac9d757ccd913aaaf\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h_params = {\n    \"epochs\":15,\n    \"learning_rate\":0.0001,\n    \"batch_size\":256,\n    \"actv_func\":\"relu\",\n    \"data_augumentation\":True,\n    \"dropout\":0.2,\n    \"fc_layer_size\":50,\n    \"model\":\"resnet50\",\n    \"last_unfreeze_layers\":1\n}\n\nIMAGE_SIZE = 299\nNUM_OF_CLASSES = 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_dataset_with_class_distribution(dataset, split_ratio):\n    train_indices = []\n    val_indices = []\n\n    # Hardcoded class ranges based on the provided dataset\n    class_ranges = [\n        (0, 999),\n        (1000, 1999),\n        (2000, 2999),\n        (3000, 3999),\n        (4000, 4998),\n        (4999, 5998),\n        (5999, 6998),\n        (6999, 7998),\n        (7999, 8998),\n        (8999, 9998)\n    ]\n\n    for start, end in class_ranges:\n        class_indices = list(range(start, end + 1))\n        split_idx = int(len(class_indices) * split_ratio)\n        train_indices.extend(class_indices[:split_idx])\n        val_indices.extend(class_indices[split_idx:])\n\n    train_dataset = Subset(dataset, train_indices)\n    val_dataset = Subset(dataset, val_indices)\n    \n    return train_dataset, val_dataset\n\ndef prepare_data(h_params):\n    desired_size = (IMAGE_SIZE, IMAGE_SIZE)\n\n    if h_params[\"data_augumentation\"]:\n        train_transform = transforms.Compose([\n            transforms.Resize(desired_size),  \n             transforms.RandomHorizontalFlip(),\n            transforms.RandomVerticalFlip(),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n            transforms.GaussianBlur(kernel_size=3),\n            transforms.ToTensor()        \n        ])\n    else:\n         train_transform = transforms.Compose([\n            transforms.Resize(desired_size),  \n            transforms.ToTensor()        \n        ])\n\n    test_transform = transforms.Compose([\n        transforms.Resize(desired_size),  \n        transforms.ToTensor()        \n    ])\n\n    train_data_dir = \"/kaggle/input/nature1/inaturalist_12K/train\"\n    test_data_dir = \"/kaggle/input/nature1/inaturalist_12K/val\"\n    train_dataset_total = ImageFolder(train_data_dir, transform=train_transform)\n    train_dataset, validation_dataset = split_dataset_with_class_distribution(train_dataset_total, 0.8)\n\n    test_dataset = ImageFolder(test_data_dir, transform=test_transform)\n    train_len = len(train_dataset)\n    val_len = len(validation_dataset)\n    test_len = len(test_dataset)\n\n    batch_size =h_params[\"batch_size\"]\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n    test_loader =  DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\n    # Return the datasets, loaders, and transforms as a dictionary\n    return {\n        \"train_len\": train_len,\n        \"val_len\": val_len,\n        \"test_len\": test_len,\n        \"train_loader\": train_loader,\n        \"val_loader\": val_loader,\n        \"test_loader\": test_loader\n    }\n\n\n\n#Preparing the data\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-05T04:18:47.339461Z","iopub.execute_input":"2024-04-05T04:18:47.339866Z","iopub.status.idle":"2024-04-05T04:18:47.359068Z","shell.execute_reply.started":"2024-04-05T04:18:47.339834Z","shell.execute_reply":"2024-04-05T04:18:47.357892Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def inceptionV3Model(h_params):\n    model = models.inception_v3(pretrained=True)\n    num_ftrs = model.fc.in_features\n    model.fc = torch.nn.Linear(num_ftrs, NUM_OF_CLASSES)\n\n    for param in model.parameters():\n    #         print(param)\n            param.requires_grad = False\n\n    # Unfreeze parameters of the fc layer\n    for param in model.fc.parameters():\n        param.requires_grad = True\n    return model\n\ndef resnet50Model(h_params):\n    model = models.resnet50(pretrained=True)\n#     print(model)\n    num_ftrs = model.fc.in_features\n    model.fc = torch.nn.Linear(num_ftrs, NUM_OF_CLASSES)\n\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Unfreeze parameters of the fc layer\n    for param in model.fc.parameters():\n        param.requires_grad = True\n        \n    return model\n# resnet50Model(h_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass CNN(nn.Module):\n    def __init__(self, h_params):\n        super(CNN, self).__init__()\n        \n\n   \n        \n    def get_activation_function(self, activation_func_name):\n        if activation_func_name == 'elu':\n            return F.elu\n        elif activation_func_name == 'gelu':\n            return F.gelu\n        elif activation_func_name == 'silu':\n            return F.silu\n        elif activation_func_name == 'selu':\n            return F.selu\n        elif activation_func_name == 'leaky_relu':\n            return F.leaky_relu\n    \n    def neurons_in_dense_layer(self, filter_sizes, image_size):\n        for i in range(5):\n            image_size = int((image_size - filter_sizes[i] +1)/2)\n        return image_size","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-05T04:18:47.378288Z","iopub.execute_input":"2024-04-05T04:18:47.378660Z","iopub.status.idle":"2024-04-05T04:18:47.390106Z","shell.execute_reply.started":"2024-04-05T04:18:47.378636Z","shell.execute_reply":"2024-04-05T04:18:47.389139Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def train(h_params, training_data):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = resnet50Model(h_params)\n    model = torch.nn.DataParallel(model, device_ids = [0,1]).to(device)\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=h_params[\"learning_rate\"])\n    train_len = training_data['train_len']\n    val_len =  training_data['val_len']\n    train_loader = training_data['train_loader']\n    val_loader = training_data['val_loader']\n\n    for epoch in range(h_params[\"epochs\"]):\n        training_loss = 0.0\n        validation_loss = 0.0\n        train_correct = 0\n        validation_correct = 0\n         # Training phase\n        model.train()\n        for i, data in enumerate(train_loader, 0):\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = loss_fn(outputs, labels)\n            training_loss+=loss.item()\n            values, predicted = torch.max(outputs, 1)\n            crt = (predicted == labels).sum().item() \n            train_correct += crt\n            loss.backward()\n            optimizer.step()\n            if (i%10 == 0):\n                print( \"                            epoch  \", epoch, \" batch \", i, \" accuracy \", crt/labels.shape[0], \" loss \", loss.item())\n\n          \n        # Validation phase\n        model.eval()\n        with torch.no_grad():\n            for data in val_loader:\n                inputs, labels = data\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                outputs = model(inputs)\n                loss = loss_fn(outputs, labels)\n\n                values, predicted = torch.max(outputs, 1)\n                validation_correct += (predicted == labels).sum().item()\n                validation_loss += loss.item()\n        \n        train_accuracy = train_correct/train_len\n        train_loss  = training_loss/len(train_loader)\n        validation_accuracy = validation_correct/val_len\n        validation_loss = validation_loss/len(val_loader)\n        print(\"epoch: \", epoch, \"train accuray:\",train_accuracy , \"train loss:\",train_loss , \"val accuracy:\", validation_accuracy,\"val loss:\",validation_loss)\n        \n        #logging to wandb\n        wandb.log({\"train_accuracy\":train_accuracy, \"train_loss\":train_loss, \"val_accuracy\":validation_accuracy, \"val_loss\":validation_loss, \"epoch\":epoch})\n\n\n    print('Finished Training')\n    PATH = './model.pth'\n    torch.save(model.state_dict(), PATH)\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-05T04:18:47.444404Z","iopub.execute_input":"2024-04-05T04:18:47.445195Z","iopub.status.idle":"2024-04-05T04:18:47.461355Z","shell.execute_reply.started":"2024-04-05T04:18:47.445170Z","shell.execute_reply":"2024-04-05T04:18:47.460144Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"config = h_params\nconfig = {\n    \"epochs\":15,\n    \"learning_rate\":0.0001,\n    \"batch_size\":256,\n    \"actv_func\":\"relu\",\n    \"data_augumentation\":True,\n    \"dropout\":0.2,\n    \"fc_layer_size\":50,\n    \"model\":\"InceptionV3\",\n    \"last_unfreeze_layers\":1\n}\n\n\nstart_time = time.time()  # Record the start time\ntraining_data = prepare_data(config)\nrun = wandb.init(project=\"DL Assignment 2B\", name=f\"{config['model']}_ep_{config['epochs']}_lr_{config['learning_rate']}_last_unfreeze_layers_{config['last_unfreeze_layers']}_data_aug_{config['data_augumentation']}\", config=config)\ntrain(config, training_data) \nend_time = time.time()  # Record the end time\ntraining_time = end_time - start_time\nprint(f\"  Training time: {training_time/60} minutes , per epoch:{training_time/(60*10)} minutes\")","metadata":{"execution":{"iopub.status.busy":"2024-04-05T04:18:47.463744Z","iopub.execute_input":"2024-04-05T04:18:47.464148Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sweep_params = {\n#     'method' : 'bayes',\n#     'name'   : 'DL assn 2 sweep',\n#     'metric' : {\n#         'goal' : 'maximize',\n#         'name' : 'val_accuracy',\n#     },\n#     'parameters' : {\n#         'epochs':{'values' : [10]},\n#         'learning_rate':{'values' : [0.0001, 0.001]},\n#         'batch_size':{'values':[128, 256, 512]},\n#         'num_of_filter':{'values' : [16,32,64] } ,\n#         'filter_size':{'values' : [[3,3,3,3,3], [5,5,5,5,5], [7,7,7,7,7], [11,9,7,5,3], [3,5,7,9,11]]},\n#         'actv_func':{'values':['elu','gelu','leaky_relu','selu']},\n#         'filter_multiplier':{'values' : [1,2,0.5]},\n#         'data_augumentation':{'values': [True, False]},\n#         'batch_normalization':{'values' : [True, False]},\n#         'dropout':{'values': [0,0.1,0.2]},\n#         'dense_layer_size':{'values' : [64, 128,256]},\n#         'conv_layers':{'values':[5]}\n#     }\n# }\n\n# sweep_id = wandb.sweep(sweep=sweep_params, project=\"DL Assignment 2B\")\n# print(sweep_id)\n# def main():\n#     wandb.init(project=\"DL Assignment 2B\" )\n#     config = wandb.config\n#     start_time = time.time()  # Record the start time\n#     with wandb.init(project=\"DL Assignment 2B\", name=f\"{config['actv_func']}_ep_{config['epochs']}_lr_{config['learning_rate']}_init_fltr_cnt_{config['num_of_filter']}_fltr_sz_{config['filter_size']}_fltr_mult_{config['filter_multiplier']}_data_aug_{config['data_augumentation']}_batch_norm_{config['batch_normalization']}_dropout_{config['dropout']}_dense_size_{config['dense_layer_size']}_batch_size_{config['batch_size']}\", config=config ):\n#         training_data = prepare_data(config)\n#         train(config,training_data)\n#     end_time = time.time()  # Record the end time\n#     training_time = end_time - start_time\n#     print(f\"  Training time: {training_time/60} minutes , per epoch:{training_time/(60*10)} minutes\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wandb.agent(\"iji9tba8\", function=main, count=50)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}