{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import torch\n","from torchvision.datasets import ImageFolder\n","import torchvision.transforms as transforms\n","from torch.utils.data import Subset, DataLoader\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import wandb\n","import time\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["wandb.login(key=\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#configure the hyper parameters here\n","h_params =  {\n","    \"epochs\":10,\n","    \"learning_rate\":0.0001,\n","    \"batch_size\":32,\n","    \"num_of_filter\":64,\n","    \"filter_size\":[3,3,3,3,3],\n","    \"actv_func\":\"gelu\",\n","    \"filter_multiplier\":2,\n","    \"data_augumentation\":False,\n","    \"batch_normalization\":True,\n","    \"dropout\":0.4,\n","    \"conv_layers\":5,\n","    \"dense_layer_size\":256\n","}\n","\n","\n","IMAGE_SIZE = 224\n","NUM_OF_CLASSES = 10"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["#This method will spilt the training data into training and validation data\n","def split_dataset_with_class_distribution(dataset, split_ratio):\n","    train_indices = []\n","    val_indices = []\n","\n","    # Hardcoded class ranges based on the provided dataset\n","    class_ranges = [\n","        (0, 999),\n","        (1000, 1999),\n","        (2000, 2999),\n","        (3000, 3999),\n","        (4000, 4998),\n","        (4999, 5998),\n","        (5999, 6998),\n","        (6999, 7998),\n","        (7999, 8998),\n","        (8999, 9998)\n","    ]\n","\n","    for start, end in class_ranges:\n","        class_indices = list(range(start, end + 1))\n","        split_idx = int(len(class_indices) * split_ratio)\n","        train_indices.extend(class_indices[:split_idx])\n","        val_indices.extend(class_indices[split_idx:])\n","\n","    train_dataset = Subset(dataset, train_indices)\n","    val_dataset = Subset(dataset, val_indices)\n","    \n","    return train_dataset, val_dataset\n","\n","\n","#This method will generate train , validation, test data and returns it\n","def prepare_data(h_params):\n","    desired_size = (IMAGE_SIZE, IMAGE_SIZE)\n","\n","    if h_params[\"data_augumentation\"]:\n","        train_transform = transforms.Compose([\n","            transforms.Resize(desired_size),  \n","             transforms.RandomHorizontalFlip(),\n","            transforms.RandomVerticalFlip(),\n","            transforms.RandomRotation(10),\n","            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","            transforms.GaussianBlur(kernel_size=3),\n","            transforms.ToTensor()        \n","        ])\n","    else:\n","         train_transform = transforms.Compose([\n","            transforms.Resize(desired_size),  \n","            transforms.ToTensor()        \n","        ])\n","\n","    test_transform = transforms.Compose([\n","        transforms.Resize(desired_size),  \n","        transforms.ToTensor()        \n","    ])\n","\n","    train_data_dir = \"/kaggle/input/nature/inaturalist_12K/train\"\n","    test_data_dir = \"/kaggle/input/nature/inaturalist_12K/val\"\n","    train_dataset_total = ImageFolder(train_data_dir, transform=train_transform)\n","    train_dataset, validation_dataset = split_dataset_with_class_distribution(train_dataset_total, 0.8)\n","\n","    test_dataset = ImageFolder(test_data_dir, transform=test_transform)\n","    train_len = len(train_dataset)\n","    val_len = len(validation_dataset)\n","    test_len = len(test_dataset)\n","\n","    batch_size =h_params[\"batch_size\"]\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n","    test_loader =  DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","    # Return the datasets, loaders, and transforms as a dictionary\n","    return {\n","        \"train_len\": train_len,\n","        \"val_len\": val_len,\n","        \"test_len\": test_len,\n","        \"train_loader\": train_loader,\n","        \"val_loader\": val_loader,\n","        \"test_loader\": test_loader\n","    }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#This is the class to build the CNN model\n","class CNN(nn.Module):\n","    \n","    #Initialization method\n","    def __init__(self, h_params):\n","        super(CNN, self).__init__()\n","        self.h_params = h_params\n","        \n","        #create the no of fileters based on the multiplier\n","        self.filters =  []\n","        for i in range(5):\n","            self.filters.append(int(self.h_params[\"num_of_filter\"]*(self.h_params[\"filter_multiplier\"]**i)))\n","        print(self.filters)\n","        \n","        # Define convolutional layers\n","        self.conv_layers = nn.ModuleList()\n","        self.bn_layers = nn.ModuleList()\n","        in_channels = 3\n","        for i in range(self.h_params[\"conv_layers\"]):\n","            conv_layer = nn.Conv2d(in_channels, self.filters[i], self.h_params[\"filter_size\"][i])\n","            self.conv_layers.append(conv_layer)\n","            if self.h_params[\"batch_normalization\"]:\n","                self.bn_layers.append(nn.BatchNorm2d(self.filters[i]))\n","            in_channels = self.filters[i]\n","        \n","        #Define Fully connected layers\n","        f_map_side = self.neurons_in_dense_layer(self.h_params[\"filter_size\"], IMAGE_SIZE)\n","        self.fc1 = nn.Linear( self.filters[-1]*f_map_side*f_map_side , self.h_params[\"dense_layer_size\"])\n","        self.fc2 = nn.Linear(self.h_params[\"dense_layer_size\"], NUM_OF_CLASSES)\n","        self.activation_func = self.get_activation_function(self.h_params[\"actv_func\"])\n","        # Define dropout layer\n","        self.dropout = nn.Dropout(p=self.h_params[\"dropout\"])\n","\n","        \n","    #forward propogate method\n","    def forward(self, x):\n","        for i in range(self.h_params[\"conv_layers\"]):\n","            x = self.conv_layers[i](x)\n","            if self.h_params[\"batch_normalization\"]:\n","                x = self.bn_layers[i](x)\n","            x = self.activation_func(x)\n","            x = F.max_pool2d(x, 2, 2)\n","        \n","        # Flatten\n","        x = x.view(x.size(0), -1)\n","        \n","        # Fully connected layers\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x\n","        \n","    #Returns appropriate activation function\n","    def get_activation_function(self, activation_func_name):\n","        if activation_func_name == 'elu':\n","            return F.elu\n","        elif activation_func_name == 'gelu':\n","            return F.gelu\n","        elif activation_func_name == 'silu':\n","            return F.silu\n","        elif activation_func_name == 'selu':\n","            return F.selu\n","        elif activation_func_name == 'leaky_relu':\n","            return F.leaky_relu\n","    \n","    #Return the number of neurons that should be in the dense layer\n","    def neurons_in_dense_layer(self, filter_sizes, image_size):\n","        for i in range(5):\n","            image_size = int((image_size - filter_sizes[i] +1)/2)\n","        return image_size"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#To evaluate the model on testing data set\n","def evaluate_testing_model(model,device, loader_data):\n","    # Set the model to evaluation mode\n","    model.eval()\n","    correct = 0\n","    test_loader =  loader_data[\"test_loader\"]\n","    with torch.no_grad():  # Disable gradient calculation to speed up computations\n","      for inputs, labels in test_loader:\n","          inputs, labels = inputs.to(device), labels.to(device)\n","            \n","          outputs = model(inputs)\n","\n","          values, predicted = torch.max(outputs, 1)\n","\n","          correct += (predicted == labels).sum().item()\n","\n","    # Calculate accuracy\n","    accuracy = correct / loader_data[\"test_len\"]\n","\n","    print(\"Test accuracy: \", accuracy)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","#To generate the 10*3 grid\n","def generateGridImage(model, device, loader_data):\n","\n","    # Initialize lists to store true labels, predicted labels, and images\n","    class_label_names = [\"Amphibia\", \"Animalia\", \"Arachnida\", \"Aves\", \"Fungi\", \"Insecta\", \"Mammalia\", \"Mollusca\", \"Plantae\", \"Reptilia\"]\n","    true_labels = []\n","    predicted_labels = []\n","    images = []\n","\n","    # Get the first 30 data items from the test dataset\n","    test_loader = loader_data['test_loader']\n","    data_iterator = iter(test_loader)\n","    for i in range(30):\n","        inputs, labels = next(data_iterator)\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        # Forward pass\n","        outputs = model(inputs)\n","\n","        # Compute predicted labels\n","        _, predicted = torch.max(outputs, 1)\n","\n","        # Append true and predicted labels to lists\n","        true_labels.extend(labels.cpu().numpy())\n","        predicted_labels.extend(predicted.cpu().numpy())\n","\n","        # Append images to the list after converting them from tensor to numpy array\n","        images.extend(inputs.cpu().numpy())\n","\n","    # Create a grid of images with true and predicted labels\n","    fig, axs = plt.subplots(10, 3, figsize=(15, 50))\n","\n","    for i, ax in enumerate(axs.flatten()):\n","        ax.imshow(np.transpose(images[i], (1, 2, 0)))  # Convert from CHW to HWC format\n","        ax.axis('off')\n","        true_label_name = class_label_names[true_labels[i]]\n","        predicted_label_name = class_label_names[predicted_labels[i]]\n","        ax.set_title(f'\\n\\n\\nTrue:{true_label_name}\\nPredicted:{predicted_label_name}')\n","\n","    plt.tight_layout()\n","\n","    # Convert the plot to a wandb image\n","    wandb_image = wandb.Image(plt)\n","\n","    # Log the image to wandb\n","    wandb.log({\"Predictions\": wandb_image})\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["def train(h_params, training_data):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = CNN(h_params)\n","    model = torch.nn.DataParallel(model, device_ids = [0,1]).to(device)\n","    loss_fn = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=h_params[\"learning_rate\"])\n","    train_len = training_data['train_len']\n","    val_len =  training_data['val_len']\n","    train_loader = training_data['train_loader']\n","    val_loader = training_data['val_loader']\n","\n","    for epoch in range(h_params[\"epochs\"]):\n","        training_loss = 0.0\n","        validation_loss = 0.0\n","        train_correct = 0\n","        validation_correct = 0\n","         # Training phase\n","        model.train()\n","        for i, data in enumerate(train_loader, 0):\n","            inputs, labels = data\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = loss_fn(outputs, labels)\n","            training_loss+=loss.item()\n","            values, predicted = torch.max(outputs, 1)\n","            crt = (predicted == labels).sum().item() \n","            train_correct += crt\n","            loss.backward()\n","            optimizer.step()\n","            if (i%10 == 0):\n","                print( \"epoch  \", epoch, \" batch \", i, \" accuracy \", crt/labels.shape[0], \" loss \", loss.item())\n","\n","          \n","        # Validation phase\n","        model.eval()\n","        with torch.no_grad():\n","            for data in val_loader:\n","                inputs, labels = data\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                outputs = model(inputs)\n","                loss = loss_fn(outputs, labels)\n","\n","                values, predicted = torch.max(outputs, 1)\n","                validation_correct += (predicted == labels).sum().item()\n","                validation_loss += loss.item()\n","        \n","        train_accuracy = train_correct/train_len\n","        train_loss  = training_loss/len(train_loader)\n","        validation_accuracy = validation_correct/val_len\n","        validation_loss = validation_loss/len(val_loader)\n","        print(\"epoch: \", epoch, \"train accuray:\",train_accuracy , \"train loss:\",train_loss , \"val accuracy:\", validation_accuracy,\"val loss:\",validation_loss)\n","        \n","        #logging to wandb\n","        wandb.log({\"train_accuracy\":train_accuracy, \"train_loss\":train_loss, \"val_accuracy\":validation_accuracy, \"val_loss\":validation_loss, \"epoch\":epoch})\n","    \n","#     Uncomment below line to find the model performance on test data\n","#     evaluate_testing_model(model,device,training_data)\n","    \n","#     Uncomment below line to generate the 10*3 image grid after model is trained\n","#     generateGridImage(model,device, training_data)\n","\n","    print('Finished Training')\n","    PATH = './bestmodel.pth'\n","    torch.save(model.state_dict(), PATH)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Run this cell to train a model for a h_params configuration\n","config = h_params\n","training_data = prepare_data(config)\n","run = wandb.init(project=\"DL Assignment 2\", name=f\"{config['actv_func']}_ep_{config['epochs']}_lr_{config['learning_rate']}_init_fltr_cnt_{config['num_of_filter']}_fltr_sz_{config['filter_size']}_fltr_mult_{config['filter_multiplier']}_data_aug_{config['data_augumentation']}_batch_norm_{config['batch_normalization']}_dropout_{config['dropout']}_dense_size_{config['dense_layer_size']}\", config=config)\n","train(config, training_data) "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Run this cell to run a sweep with appropriate parameters\n","sweep_params = {\n","    'method' : 'bayes',\n","    'name'   : 'DL assn 2 sweep',\n","    'metric' : {\n","        'goal' : 'maximize',\n","        'name' : 'val_accuracy',\n","    },\n","    'parameters' : {\n","        'epochs':{'values' : [10]},\n","        'learning_rate':{'values' : [0.0001, 0.001]},\n","        'batch_size':{'values':[32,64]},\n","        'num_of_filter':{'values' : [16,32,64] } ,\n","        'filter_size':{'values' : [[3,3,3,3,3], [5,5,5,5,5], [7,7,7,7,7], [11,9,7,5,3], [3,5,7,9,11]]},\n","        'actv_func':{'values':['elu','gelu','leaky_relu','selu']},\n","        'filter_multiplier':{'values' : [1,2]},\n","        'data_augumentation':{'values': [ False]},\n","        'batch_normalization':{'values' : [True, False]},\n","        'dropout':{'values': [0,0.1,0.2]},\n","        'dense_layer_size':{'values' : [64, 128,256]},\n","        'conv_layers':{'values':[5]}\n","    }\n","}\n","\n","sweep_id = wandb.sweep(sweep=sweep_params, project=\"DL Assignment 2\")\n","def main():\n","    wandb.init(project=\"DL Assignment 2\" )\n","    config = wandb.config\n","    with wandb.init(project=\"DL Assignment 2\", name=f\"{config['actv_func']}_ep_{config['epochs']}_lr_{config['learning_rate']}_init_fltr_cnt_{config['num_of_filter']}_fltr_sz_{config['filter_size']}_fltr_mult_{config['filter_multiplier']}_data_aug_{config['data_augumentation']}_batch_norm_{config['batch_normalization']}_dropout_{config['dropout']}_dense_size_{config['dense_layer_size']}_batch_size_{config['batch_size']}\", config=config ):\n","        training_data = prepare_data(config)  \n","        train(config,training_data)\n","wandb.agent(sweep_id, function=main, count=10)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4713606,"sourceId":8003852,"sourceType":"datasetVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
