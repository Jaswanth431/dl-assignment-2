{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8003852,"sourceType":"datasetVersion","datasetId":4713606}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Subset, DataLoader\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport wandb\nimport time\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-07T05:23:31.976350Z","iopub.execute_input":"2024-04-07T05:23:31.977465Z","iopub.status.idle":"2024-04-07T05:23:31.985680Z","shell.execute_reply.started":"2024-04-07T05:23:31.977422Z","shell.execute_reply":"2024-04-07T05:23:31.984321Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"wandb.login(key=\"62cfafb7157dfba7fdd6132ac9d757ccd913aaaf\")","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:23:31.987545Z","iopub.execute_input":"2024-04-07T05:23:31.987990Z","iopub.status.idle":"2024-04-07T05:23:32.001764Z","shell.execute_reply.started":"2024-04-07T05:23:31.987954Z","shell.execute_reply":"2024-04-07T05:23:32.000856Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"h_params =  {\n    \"epochs\":10,\n    \"learning_rate\":0.0001,\n    \"batch_size\":32,\n    \"num_of_filter\":64,\n    \"filter_size\":[3,3,3,3,3],\n    \"actv_func\":\"gelu\",\n    \"filter_multiplier\":2,\n    \"data_augumentation\":False,\n    \"batch_normalization\":True,\n    \"dropout\":0.4,\n    \"conv_layers\":5,\n    \"dense_layer_size\":256\n}\n\n\nIMAGE_SIZE = 224\nNUM_OF_CLASSES = 10","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:23:32.005980Z","iopub.execute_input":"2024-04-07T05:23:32.006236Z","iopub.status.idle":"2024-04-07T05:23:32.012963Z","shell.execute_reply.started":"2024-04-07T05:23:32.006214Z","shell.execute_reply":"2024-04-07T05:23:32.011891Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"def split_dataset_with_class_distribution(dataset, split_ratio):\n    train_indices = []\n    val_indices = []\n\n    # Hardcoded class ranges based on the provided dataset\n    class_ranges = [\n        (0, 999),\n        (1000, 1999),\n        (2000, 2999),\n        (3000, 3999),\n        (4000, 4998),\n        (4999, 5998),\n        (5999, 6998),\n        (6999, 7998),\n        (7999, 8998),\n        (8999, 9998)\n    ]\n\n    for start, end in class_ranges:\n        class_indices = list(range(start, end + 1))\n        split_idx = int(len(class_indices) * split_ratio)\n        train_indices.extend(class_indices[:split_idx])\n        val_indices.extend(class_indices[split_idx:])\n\n    train_dataset = Subset(dataset, train_indices)\n    val_dataset = Subset(dataset, val_indices)\n    \n    return train_dataset, val_dataset\n\ndef prepare_data(h_params):\n    desired_size = (IMAGE_SIZE, IMAGE_SIZE)\n\n    if h_params[\"data_augumentation\"]:\n        train_transform = transforms.Compose([\n            transforms.Resize(desired_size),  \n             transforms.RandomHorizontalFlip(),\n            transforms.RandomVerticalFlip(),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n            transforms.GaussianBlur(kernel_size=3),\n            transforms.ToTensor()        \n        ])\n    else:\n         train_transform = transforms.Compose([\n            transforms.Resize(desired_size),  \n            transforms.ToTensor()        \n        ])\n\n    test_transform = transforms.Compose([\n        transforms.Resize(desired_size),  \n        transforms.ToTensor()        \n    ])\n\n    train_data_dir = \"/kaggle/input/nature/inaturalist_12K/train\"\n    test_data_dir = \"/kaggle/input/nature/inaturalist_12K/val\"\n    train_dataset_total = ImageFolder(train_data_dir, transform=train_transform)\n    train_dataset, validation_dataset = split_dataset_with_class_distribution(train_dataset_total, 0.8)\n\n    test_dataset = ImageFolder(test_data_dir, transform=test_transform)\n    train_len = len(train_dataset)\n    val_len = len(validation_dataset)\n    test_len = len(test_dataset)\n\n    batch_size =h_params[\"batch_size\"]\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n    test_loader =  DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\n    # Return the datasets, loaders, and transforms as a dictionary\n    return {\n        \"train_len\": train_len,\n        \"val_len\": val_len,\n        \"test_len\": test_len,\n        \"train_loader\": train_loader,\n        \"val_loader\": val_loader,\n        \"test_loader\": test_loader\n    }\n\n\n\n#Preparing the data\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-07T05:23:32.014860Z","iopub.execute_input":"2024-04-07T05:23:32.015184Z","iopub.status.idle":"2024-04-07T05:23:32.032745Z","shell.execute_reply.started":"2024-04-07T05:23:32.015162Z","shell.execute_reply":"2024-04-07T05:23:32.031652Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"\nclass CNN(nn.Module):\n    def __init__(self, h_params):\n        super(CNN, self).__init__()\n        self.h_params = h_params\n        \n        #create the no of fileters based on the multiplier\n        self.filters =  []\n        for i in range(5):\n            self.filters.append(int(self.h_params[\"num_of_filter\"]*(self.h_params[\"filter_multiplier\"]**i)))\n        print(self.filters)\n        \n        # Define convolutional layers\n        self.conv_layers = nn.ModuleList()\n        self.bn_layers = nn.ModuleList()\n        in_channels = 3\n        for i in range(self.h_params[\"conv_layers\"]):\n            conv_layer = nn.Conv2d(in_channels, self.filters[i], self.h_params[\"filter_size\"][i])\n            self.conv_layers.append(conv_layer)\n            if self.h_params[\"batch_normalization\"]:\n                self.bn_layers.append(nn.BatchNorm2d(self.filters[i]))\n            in_channels = self.filters[i]\n        \n        #Define Fully connected layers\n        f_map_side = self.neurons_in_dense_layer(self.h_params[\"filter_size\"], IMAGE_SIZE)\n        self.fc1 = nn.Linear( self.filters[-1]*f_map_side*f_map_side , self.h_params[\"dense_layer_size\"])\n        self.fc2 = nn.Linear(self.h_params[\"dense_layer_size\"], NUM_OF_CLASSES)\n        self.activation_func = self.get_activation_function(self.h_params[\"actv_func\"])\n        # Define dropout layer\n        self.dropout = nn.Dropout(p=self.h_params[\"dropout\"])\n\n    def forward(self, x):\n        for i in range(self.h_params[\"conv_layers\"]):\n            x = self.conv_layers[i](x)\n            if self.h_params[\"batch_normalization\"]:\n                x = self.bn_layers[i](x)\n            x = self.activation_func(x)\n            x = F.max_pool2d(x, 2, 2)\n        \n        # Flatten\n        x = x.view(x.size(0), -1)\n        \n        # Fully connected layers\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n        \n    def get_activation_function(self, activation_func_name):\n        if activation_func_name == 'elu':\n            return F.elu\n        elif activation_func_name == 'gelu':\n            return F.gelu\n        elif activation_func_name == 'silu':\n            return F.silu\n        elif activation_func_name == 'selu':\n            return F.selu\n        elif activation_func_name == 'leaky_relu':\n            return F.leaky_relu\n    \n    def neurons_in_dense_layer(self, filter_sizes, image_size):\n        for i in range(5):\n            image_size = int((image_size - filter_sizes[i] +1)/2)\n        return image_size","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:23:32.034161Z","iopub.execute_input":"2024-04-07T05:23:32.034731Z","iopub.status.idle":"2024-04-07T05:23:32.055732Z","shell.execute_reply.started":"2024-04-07T05:23:32.034701Z","shell.execute_reply":"2024-04-07T05:23:32.054805Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def evaluate_testing_model(model,device, loader_data):\n    # Set the model to evaluation mode\n    model.eval()\n    correct = 0\n    test_loader =  loader_data[\"test_loader\"]\n    with torch.no_grad():  # Disable gradient calculation to speed up computations\n      for inputs, labels in test_loader:\n          inputs, labels = inputs.to(device), labels.to(device)\n            \n          outputs = model(inputs)\n\n          values, predicted = torch.max(outputs, 1)\n\n          correct += (predicted == labels).sum().item()\n\n    # Calculate accuracy\n    accuracy = correct / loader_data[\"test_len\"]\n\n    print(\"Test accuracy: \", accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:23:32.176809Z","iopub.execute_input":"2024-04-07T05:23:32.177284Z","iopub.status.idle":"2024-04-07T05:23:32.183912Z","shell.execute_reply.started":"2024-04-07T05:23:32.177253Z","shell.execute_reply":"2024-04-07T05:23:32.183058Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def generateGridImage(model, device, loader_data):\n\n    # Initialize lists to store true labels, predicted labels, and images\n    class_label_names = [\"Amphibia\", \"Animalia\", \"Arachnida\", \"Aves\", \"Fungi\", \"Insecta\", \"Mammalia\", \"Mollusca\", \"Plantae\", \"Reptilia\"]\n    true_labels = []\n    predicted_labels = []\n    images = []\n\n    # Get the first 30 data items from the test dataset\n    test_loader = loader_data['test_loader']\n    data_iterator = iter(test_loader)\n    for i in range(30):\n        inputs, labels = next(data_iterator)\n        inputs, labels = inputs.to(device), labels.to(device)\n        # Forward pass\n        outputs = model(inputs)\n\n        # Compute predicted labels\n        _, predicted = torch.max(outputs, 1)\n\n        # Append true and predicted labels to lists\n        true_labels.extend(labels.cpu().numpy())\n        predicted_labels.extend(predicted.cpu().numpy())\n\n        # Append images to the list after converting them from tensor to numpy array\n        images.extend(inputs.cpu().numpy())\n\n    # Create a grid of images with true and predicted labels\n    fig, axs = plt.subplots(10, 3, figsize=(15, 50))\n\n    for i, ax in enumerate(axs.flatten()):\n        ax.imshow(np.transpose(images[i], (1, 2, 0)))  # Convert from CHW to HWC format\n        ax.axis('off')\n        true_label_name = class_label_names[true_labels[i]]\n        predicted_label_name = class_label_names[predicted_labels[i]]\n        ax.set_title(f'\\n\\n\\nTrue:{true_label_name}\\nPredicted:{predicted_label_name}')\n\n    plt.tight_layout()\n\n    # Convert the plot to a wandb image\n    wandb_image = wandb.Image(plt)\n\n    # Log the image to wandb\n    wandb.log({\"Predictions\": wandb_image})\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:23:32.185534Z","iopub.execute_input":"2024-04-07T05:23:32.185922Z","iopub.status.idle":"2024-04-07T05:23:32.198516Z","shell.execute_reply.started":"2024-04-07T05:23:32.185897Z","shell.execute_reply":"2024-04-07T05:23:32.197604Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"def train(h_params, training_data):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = CNN(h_params)\n    model = torch.nn.DataParallel(model, device_ids = [0,1]).to(device)\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=h_params[\"learning_rate\"])\n    train_len = training_data['train_len']\n    val_len =  training_data['val_len']\n    train_loader = training_data['train_loader']\n    val_loader = training_data['val_loader']\n\n    for epoch in range(h_params[\"epochs\"]):\n        training_loss = 0.0\n        validation_loss = 0.0\n        train_correct = 0\n        validation_correct = 0\n         # Training phase\n        model.train()\n        for i, data in enumerate(train_loader, 0):\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = loss_fn(outputs, labels)\n            training_loss+=loss.item()\n            values, predicted = torch.max(outputs, 1)\n            crt = (predicted == labels).sum().item() \n            train_correct += crt\n            loss.backward()\n            optimizer.step()\n            if (i%10 == 0):\n                print( \"                            epoch  \", epoch, \" batch \", i, \" accuracy \", crt/labels.shape[0], \" loss \", loss.item())\n\n          \n        # Validation phase\n        model.eval()\n        with torch.no_grad():\n            for data in val_loader:\n                inputs, labels = data\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                outputs = model(inputs)\n                loss = loss_fn(outputs, labels)\n\n                values, predicted = torch.max(outputs, 1)\n                validation_correct += (predicted == labels).sum().item()\n                validation_loss += loss.item()\n        \n        train_accuracy = train_correct/train_len\n        train_loss  = training_loss/len(train_loader)\n        validation_accuracy = validation_correct/val_len\n        validation_loss = validation_loss/len(val_loader)\n        print(\"epoch: \", epoch, \"train accuray:\",train_accuracy , \"train loss:\",train_loss , \"val accuracy:\", validation_accuracy,\"val loss:\",validation_loss)\n        \n        #logging to wandb\n        wandb.log({\"train_accuracy\":train_accuracy, \"train_loss\":train_loss, \"val_accuracy\":validation_accuracy, \"val_loss\":validation_loss, \"epoch\":epoch})\n    evaluate_testing_model(model,device,training_data)\n    generateGridImage(model,device, training_data)\n\n    print('Finished Training')\n    PATH = './bestmodel.pth'\n    torch.save(model.state_dict(), PATH)\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-07T05:23:32.210402Z","iopub.execute_input":"2024-04-07T05:23:32.210671Z","iopub.status.idle":"2024-04-07T05:23:32.227068Z","shell.execute_reply.started":"2024-04-07T05:23:32.210650Z","shell.execute_reply":"2024-04-07T05:23:32.226136Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"config = h_params\n\nstart_time = time.time()  # Record the start time\ntraining_data = prepare_data(config)\nrun = wandb.init(project=\"DL Assignment 2\", name=f\"{config['actv_func']}_ep_{config['epochs']}_lr_{config['learning_rate']}_init_fltr_cnt_{config['num_of_filter']}_fltr_sz_{config['filter_size']}_fltr_mult_{config['filter_multiplier']}_data_aug_{config['data_augumentation']}_batch_norm_{config['batch_normalization']}_dropout_{config['dropout']}_dense_size_{config['dense_layer_size']}\", config=config)\ntrain(config, training_data) \nend_time = time.time()  # Record the end time\ntraining_time = end_time - start_time\nprint(f\"  Training time: {training_time/60} minutes , per epoch:{training_time/(60*10)} minutes\")","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:23:32.229041Z","iopub.execute_input":"2024-04-07T05:23:32.229653Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:gqpt0p2u) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='6.779 MB of 6.779 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train_accuracy</td><td>0.2164</td></tr><tr><td>train_loss</td><td>2.16159</td></tr><tr><td>val_accuracy</td><td>0.2895</td></tr><tr><td>val_loss</td><td>2.05195</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">gelu_ep_1_lr_0.0001_init_fltr_cnt_64_fltr_sz_[3, 3, 3, 3, 3]_fltr_mult_2_data_aug_False_batch_norm_True_dropout_0.4_dense_size_256</strong> at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%202/runs/gqpt0p2u' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%202/runs/gqpt0p2u</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240407_051856-gqpt0p2u/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:gqpt0p2u). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240407_052333-eg8py6ad</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/jaswanth431/DL%20Assignment%202/runs/eg8py6ad' target=\"_blank\">gelu_ep_10_lr_0.0001_init_fltr_cnt_64_fltr_sz_[3, 3, 3, 3, 3]_fltr_mult_2_data_aug_False_batch_norm_True_dropout_0.4_dense_size_256</a></strong> to <a href='https://wandb.ai/jaswanth431/DL%20Assignment%202' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%202' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%202</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%202/runs/eg8py6ad' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%202/runs/eg8py6ad</a>"},"metadata":{}},{"name":"stdout","text":"[64, 128, 256, 512, 1024]\n                            epoch   0  batch  0  accuracy  0.09375  loss  2.3913660049438477\n                            epoch   0  batch  10  accuracy  0.125  loss  2.345008611679077\n                            epoch   0  batch  20  accuracy  0.25  loss  2.071932077407837\n                            epoch   0  batch  30  accuracy  0.125  loss  2.2595014572143555\n                            epoch   0  batch  40  accuracy  0.15625  loss  2.203084707260132\n                            epoch   0  batch  50  accuracy  0.21875  loss  2.352325201034546\n                            epoch   0  batch  60  accuracy  0.3125  loss  2.1434834003448486\n                            epoch   0  batch  70  accuracy  0.1875  loss  2.006237268447876\n                            epoch   0  batch  80  accuracy  0.09375  loss  2.3829150199890137\n                            epoch   0  batch  90  accuracy  0.125  loss  2.1908352375030518\n                            epoch   0  batch  100  accuracy  0.28125  loss  2.1403939723968506\n                            epoch   0  batch  110  accuracy  0.21875  loss  2.1476197242736816\n                            epoch   0  batch  120  accuracy  0.34375  loss  2.046961545944214\n                            epoch   0  batch  130  accuracy  0.15625  loss  2.2197957038879395\n                            epoch   0  batch  140  accuracy  0.25  loss  2.1028504371643066\n                            epoch   0  batch  150  accuracy  0.4375  loss  1.8041410446166992\n                            epoch   0  batch  160  accuracy  0.3125  loss  1.9980703592300415\n                            epoch   0  batch  170  accuracy  0.15625  loss  2.376723289489746\n                            epoch   0  batch  180  accuracy  0.25  loss  2.232961654663086\n                            epoch   0  batch  190  accuracy  0.1875  loss  2.250669240951538\n                            epoch   0  batch  200  accuracy  0.28125  loss  1.9104235172271729\n                            epoch   0  batch  210  accuracy  0.21875  loss  2.048365592956543\n                            epoch   0  batch  220  accuracy  0.1875  loss  2.376681089401245\n                            epoch   0  batch  230  accuracy  0.3125  loss  2.004545211791992\n                            epoch   0  batch  240  accuracy  0.3125  loss  2.1313037872314453\nepoch:  0 train accuray: 0.22815351918989873 train loss: 2.1511167769432067 val accuracy: 0.294 val loss: 2.0122238908495222\n                            epoch   1  batch  0  accuracy  0.125  loss  2.205141305923462\n                            epoch   1  batch  10  accuracy  0.3125  loss  1.910392165184021\n                            epoch   1  batch  20  accuracy  0.1875  loss  2.0638647079467773\n                            epoch   1  batch  30  accuracy  0.34375  loss  2.0408029556274414\n                            epoch   1  batch  40  accuracy  0.09375  loss  2.0600879192352295\n                            epoch   1  batch  50  accuracy  0.25  loss  2.0010132789611816\n                            epoch   1  batch  60  accuracy  0.28125  loss  2.161039352416992\n                            epoch   1  batch  70  accuracy  0.59375  loss  1.6723682880401611\n                            epoch   1  batch  80  accuracy  0.40625  loss  1.8499610424041748\n                            epoch   1  batch  90  accuracy  0.21875  loss  1.9464353322982788\n                            epoch   1  batch  100  accuracy  0.21875  loss  2.0534207820892334\n                            epoch   1  batch  110  accuracy  0.28125  loss  2.0686092376708984\n                            epoch   1  batch  120  accuracy  0.3125  loss  1.9332950115203857\n                            epoch   1  batch  130  accuracy  0.4375  loss  1.8767210245132446\n                            epoch   1  batch  140  accuracy  0.15625  loss  2.1501386165618896\n                            epoch   1  batch  150  accuracy  0.28125  loss  2.1968533992767334\n                            epoch   1  batch  160  accuracy  0.25  loss  2.1705148220062256\n                            epoch   1  batch  170  accuracy  0.3125  loss  2.1142051219940186\n                            epoch   1  batch  180  accuracy  0.3125  loss  1.9009464979171753\n                            epoch   1  batch  190  accuracy  0.3125  loss  2.1684064865112305\n                            epoch   1  batch  200  accuracy  0.1875  loss  2.20586895942688\n                            epoch   1  batch  210  accuracy  0.25  loss  1.9694805145263672\n                            epoch   1  batch  220  accuracy  0.28125  loss  2.059833526611328\n                            epoch   1  batch  230  accuracy  0.34375  loss  1.9266749620437622\n                            epoch   1  batch  240  accuracy  0.125  loss  2.1781158447265625\nepoch:  1 train accuray: 0.2842855356919615 train loss: 2.014226463317871 val accuracy: 0.325 val loss: 1.9520615168980189\n                            epoch   2  batch  0  accuracy  0.28125  loss  1.8365283012390137\n                            epoch   2  batch  10  accuracy  0.34375  loss  2.084120273590088\n                            epoch   2  batch  20  accuracy  0.21875  loss  2.070713520050049\n                            epoch   2  batch  30  accuracy  0.21875  loss  2.172618865966797\n                            epoch   2  batch  40  accuracy  0.4375  loss  1.587282419204712\n                            epoch   2  batch  50  accuracy  0.5  loss  1.7190603017807007\n                            epoch   2  batch  60  accuracy  0.1875  loss  2.000378370285034\n                            epoch   2  batch  70  accuracy  0.21875  loss  1.9355193376541138\n                            epoch   2  batch  80  accuracy  0.3125  loss  1.843276023864746\n                            epoch   2  batch  90  accuracy  0.3125  loss  1.915487289428711\n                            epoch   2  batch  100  accuracy  0.34375  loss  1.895145058631897\n                            epoch   2  batch  110  accuracy  0.21875  loss  2.0601608753204346\n                            epoch   2  batch  120  accuracy  0.3125  loss  2.105611801147461\n                            epoch   2  batch  130  accuracy  0.28125  loss  1.996649980545044\n                            epoch   2  batch  140  accuracy  0.25  loss  2.171170711517334\n                            epoch   2  batch  150  accuracy  0.40625  loss  1.8895033597946167\n                            epoch   2  batch  160  accuracy  0.40625  loss  1.6920775175094604\n                            epoch   2  batch  170  accuracy  0.4375  loss  1.8647540807724\n                            epoch   2  batch  180  accuracy  0.28125  loss  2.0962884426116943\n                            epoch   2  batch  190  accuracy  0.09375  loss  2.2303202152252197\n                            epoch   2  batch  200  accuracy  0.34375  loss  1.835641622543335\n                            epoch   2  batch  210  accuracy  0.4375  loss  1.6241872310638428\n                            epoch   2  batch  220  accuracy  0.3125  loss  2.2297005653381348\n                            epoch   2  batch  230  accuracy  0.1875  loss  1.959719181060791\n                            epoch   2  batch  240  accuracy  0.28125  loss  1.9146018028259277\nepoch:  2 train accuray: 0.31728966120765095 train loss: 1.9471892085075377 val accuracy: 0.3385 val loss: 1.8872123464705453\n                            epoch   3  batch  0  accuracy  0.40625  loss  1.7670514583587646\n                            epoch   3  batch  10  accuracy  0.375  loss  1.8274670839309692\n                            epoch   3  batch  20  accuracy  0.28125  loss  2.0220606327056885\n                            epoch   3  batch  30  accuracy  0.3125  loss  1.9617364406585693\n                            epoch   3  batch  40  accuracy  0.25  loss  1.8752031326293945\n                            epoch   3  batch  50  accuracy  0.34375  loss  1.8649033308029175\n                            epoch   3  batch  60  accuracy  0.34375  loss  1.9804372787475586\n                            epoch   3  batch  70  accuracy  0.375  loss  1.8810263872146606\n                            epoch   3  batch  80  accuracy  0.34375  loss  1.872551679611206\n                            epoch   3  batch  90  accuracy  0.3125  loss  1.9822996854782104\n                            epoch   3  batch  100  accuracy  0.3125  loss  2.0889227390289307\n                            epoch   3  batch  110  accuracy  0.34375  loss  1.8184030055999756\n                            epoch   3  batch  120  accuracy  0.375  loss  2.014284133911133\n                            epoch   3  batch  130  accuracy  0.3125  loss  1.8749881982803345\n                            epoch   3  batch  140  accuracy  0.4375  loss  1.7840579748153687\n                            epoch   3  batch  150  accuracy  0.3125  loss  1.9226737022399902\n                            epoch   3  batch  160  accuracy  0.375  loss  1.9961246252059937\n                            epoch   3  batch  170  accuracy  0.21875  loss  2.2169227600097656\n                            epoch   3  batch  180  accuracy  0.3125  loss  1.867599606513977\n                            epoch   3  batch  190  accuracy  0.21875  loss  1.8738704919815063\n                            epoch   3  batch  200  accuracy  0.25  loss  1.9364644289016724\n                            epoch   3  batch  210  accuracy  0.28125  loss  1.8900055885314941\n","output_type":"stream"}]},{"cell_type":"code","source":"sweep_params = {\n    'method' : 'bayes',\n    'name'   : 'DL assn 2 sweep',\n    'metric' : {\n        'goal' : 'maximize',\n        'name' : 'val_accuracy',\n    },\n    'parameters' : {\n        'epochs':{'values' : [10]},\n        'learning_rate':{'values' : [0.0001, 0.001]},\n        'batch_size':{'values':[32,64]},\n        'num_of_filter':{'values' : [16,32,64] } ,\n        'filter_size':{'values' : [[3,3,3,3,3], [5,5,5,5,5], [7,7,7,7,7], [11,9,7,5,3], [3,5,7,9,11]]},\n        'actv_func':{'values':['elu','gelu','leaky_relu','selu']},\n        'filter_multiplier':{'values' : [1,2]},\n        'data_augumentation':{'values': [ False]},\n        'batch_normalization':{'values' : [True, False]},\n        'dropout':{'values': [0,0.1,0.2]},\n        'dense_layer_size':{'values' : [64, 128,256]},\n        'conv_layers':{'values':[5]}\n    }\n}\n\nsweep_id = wandb.sweep(sweep=sweep_params, project=\"DL Assignment 2\")\nprint(sweep_id)\ndef main():\n    wandb.init(project=\"DL Assignment 2\" )\n    config = wandb.config\n    start_time = time.time()  # Record the start time\n    with wandb.init(project=\"DL Assignment 2\", name=f\"{config['actv_func']}_ep_{config['epochs']}_lr_{config['learning_rate']}_init_fltr_cnt_{config['num_of_filter']}_fltr_sz_{config['filter_size']}_fltr_mult_{config['filter_multiplier']}_data_aug_{config['data_augumentation']}_batch_norm_{config['batch_normalization']}_dropout_{config['dropout']}_dense_size_{config['dense_layer_size']}_batch_size_{config['batch_size']}\", config=config ):\n        training_data = prepare_data(config)  \n        train(config,training_data)\n    end_time = time.time()  # Record the end time\n    training_time = end_time - start_time\n    print(f\"  Training time: {training_time/60} minutes , per epoch:{training_time/(60*10)} minutes\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wandb.agent(sweep_id, function=main, count=10)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}